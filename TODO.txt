# Priorities

Consider a very common use case for process_submissions(). I want, for all submitted answers, name, email and any answer which included http. I don't want to have to remember the weird id for all the answers that had an http. I want the function to figure that out. Really, what I want is for the function to figure out any id which has (at least 2?) answers which start with http. Then, I want that many lines for each answer. So, we have column name, email, id and answer, with missing values for any student which did not submit for that id. 

Figure out why Check is 0/0/0 but Test is 2 FAILs and 5 SKIPs.

Change format_tutorial() so it looks to see if you have any chunk names in your file with reserved names (name, email, ID, minutes, answers), and then warns you about that fact.

What other Positron settings should we set by default?

Which packages do we really need, either in Imports or in Suggests?

Check out all unit tests.

Do we need to deal with the case in which the only option is echo = FALSE. 
Right now, it just gets deleted. Which does not matter since echo = FALSE
is a global option set by all tutorials. But, this might be a sign of bad 
behavior in general. Like why delete a code chunk option under any 
circumstances? No good reason to do so! Need to fix because current code
leaves a duplicate code chunk there.

It is annoying that R attempts to run `r scales::comma(x)` even when it is placed 
in a verbatim environment. That is annoying! But how can format_tutorial() prevent? 
It can't! This is a problem with learnr::run_tutorial().

How does line-wrapping work in Positron? And, whatever the answer, is this something 
that we should set for students in the Positron tutorial?

Related to the above is that the Github Actions files differ in that R CMD check uses renv and pkgdown.yml uses dependencies. The former ensures that the quarto package is available because it is in the renv.lock file. But with dependencies, I suspect that quarto must appear in the imports part of the DESCRIPTION file, but maybe Suggests would be enough?

Do a complete clean up of the Github Action files.

Vignette about Posit Workbench.

return_type = "All" in process_submissions() should not include submission_type column

Fix code and then test case for this example: 
"~/Downloads/submissions/four-parameters-categorical_answers - Kushagra Singh.html"


Better error message if the pattern does not match any of the files in the directory.

Might be nice if the email entry field at least attempted to note/warn if the submitted information was not a legitimate email. Perhaps at least check if there is an `@`.

process_submissions() test cases should not just spit out "Could not process file: getting-started_answers.pdf". Handle it better.

I don't like the way the test cases for process_submissions spit out the pdf files names that they are ignoring, even if it does not effect the tests passing. At least, we should add a new test case for this behavior. Perhaps reorganize the test files into a couple of different directories. Or make the pattern ignore the pdf files in the test cases. But I couldn't get that to work.


What should show_file(test_file, start = 0) return? Not sure it should be an error . . .

Standardize an instruction which says to paste from the HTML result. This is different from CP/CR, obviously.

Consider getting rid of Copy code button. Now that we can pre-place code from previous questions into the Exericise code chunk, we don't need to allow students to Copy from previous exercise. Since that is the only use case for the Copy code button, we can delete it, and all its associated junk. The counter-argument is that if you populate the starting code for Exercise N + 1, then a student working on Exercise N can cheat and just look at by skipping ahead. Counter-counter-argument is that students can already do this by looking at the Hint for Exercise N + 1. Since they don't (?), we don't really have to worry about them cheating in this way. Moreover, the vast majority of questions are so easy that cheating is actually more work. Just look at the provided hint!

Consider providing a simple hash to guarantee that students haven't cheated, even if they submit html. The function would run some R code which produced a string including things like date, time, pwd, whois, computer name, and whatever else. It then takes that string and turns it into a hash. That hash is output into the html. If there are concerns about cheating, an instructor can check the hash. This might also us to get rid of the rds options, since its only purpose is probably cheat-detection. Might also provide an execuse to get rid of the PDF option.


Ensure that this code handles our test code chunks in non-code questions. See Positron and Github (gert) for examples. Seems to work, but some test cases would be nice.

Should Written Exercises with no answers have test code chunks by default? I seem to be using these a lot in 22-arrow. Most useful for the case in which the answer requires CP/CR but you still want to confirm that your code works, presumably on a simpler/quicker version of required code.

reactiveConsole(TRUE) might make it easier to experiment with reactivity in the Console.

Think harder about whether write_answer should write out questions which the student did not answer. Almost certainly Yes. Also, what about a header which include tutorial information, like name or, even id/version?

## Downloading answers

* Perhaps Section 9.2.3 Downloading reports in the Shiny book shows a much better way of handling our answer-saving problem: parameterised RMarkdown. 

* With rstudioapi, use selectFile and friends as an easier way of saving student answers.

* The **downloadthis** package looks very interesting. It seems easy to download a file without all the rigamarole of the Shiny server. But it requires that the file exists. Can we automatically run the code to create the files? Yes! We can write the code in the final exercise and then tell students to hit Run Code. They can then hit the download button. But would that really work? Isn't an Exercise code chunk its own separate world, with no notion of the tutorial of which it is a part? Maybe the same local{()} nonsense that we use submission_server() would do the trick. But that would make for some ugly code. Maybe we can hide the code, but they still press the Run Code button.


## write_answers()

* Change write_answers() completely? See https://mastering-shiny.org/action-transfer.html#downloading-reports

* See the comments at test-write_answers.R for detailed discussion. 

* We should split write_answers() into two parts. The first part just grabs everything we need from the session and returns a tibble. (This part might be too big for CRAN.) The second part takes the tibble and outputs the answers as html/pdf/rds. We can test this a lot. Do this first. The current test case will help ensure that, at least, nothing is worse.

* Change write_answers() to get rid of test mode since, with changes in exported learnr functions, this hack may no longer be necessary. 

* A different approach to the pdf file creation problem in write_answers() would be to create the Rmd document on the file as a character object rmd and then do render(text = rmd, output_file = filename).


## Points about testing

* Add a test case which ensures that downloading the answers actually works. I got caught with a bug on this when I stopped exporting write_answers(). That change caused the download buttons to fail, but I did not know it until someone reported it.

* Understand https://r-pkgs.org/testing-basics.html#sec-snapshot-tests and other advanced testing stuff. I bet that "withr::local_options" will be valuable for doing the next two tasks.

* Use testing_package() rather than hard-coding the package name.

* Read https://testthat.r-lib.org/articles/test-fixtures.html

* test_path() does not work if the file is not in tests/testthat/. Might still be useful since it means we don't need to setwd() to play with testing interactively.

* Explore tutorial testing: https://pkgs.rstudio.com/learnr/articles/exercises.html#test-code-or-cases

* Testing for make_exercise() is a mess. This might be connected to some poor design of the function itself. But it also seems like rstudioapi functions might be problematic to test for some reason.

* Do something about test-return_tutorial_paths(). 

* Maybe testing_package() or skip_if_offline() would be useful. 


## Using ___

The latest version of learnr seems to allow for ____ as a useful placeholder in code which is left in the actual exercise code blocks without creating an error. Might be useful! Might be much better than always inserting code from previous question.

So, in addition to hints, we could have each code block pre-populated with the code we want them to start with. No need for copy-code from previous exercise! But some diligent students probably don't use the hint. (But they do use Copy from Previous question.) We don't want to force them to see too much before they try out the question.

Blanks are detected using regular expressions (since blanks may make the code unparsable), and learnrâ€™s default pattern is to detect three or more consecutive underscores. Authors can choose the pattern for detecting blanks with the exercise.blanks chunk option. Setting exercise.blanks = "[.]{2}[a-z]+[.]{2}", for example, would allow the author to use valid R syntax for blanks. The warning message shown to students calls out the blanks they need to fill in.

## Other items

* Want to change the color of the OK box so that it is green when students enter
their email or other text. This seem relevant:  https://stackoverflow.com/questions/33620133/change-the-color-of-action-button-in-shiny/35871042.

* Running devtools::check(remote = TRUE, manual = TRUE) produces some spurious documentation warnings. Produce a reprex for the devtools folks.


## Get a better understanding of package locations

Keep in mind that there are (at least) two versions of **your.tutorial.package** installed on your machine. In my case (using **all.primer.tutorials**), we have

```{bash eval = FALSE}
> /Library/Frameworks/R.framework/Versions/4.1/Resources/library
```

and

```{bash eval = FALSE}
> /Users/dkane/Library/Caches/org.R-project.R/R/renv/library/all.primer.tutorials-07f29d85/R-4.2/aarch64-apple-darwin20"
```

The first is the default location for packages. This is where things go unless you do something special. The second is installed by `renv`, which was used within the **all.primer.tutorials** project. When you are working in your **all.primer.tutorials** project, as you generally will, the `renv` version of the **all.primer.tutorials** library is what you will be using. You can check this by running `.libPaths()`.

```{r eval = FALSE}
> .libPaths()
[1] "/Users/dkane/Library/Caches/org.R-project.R/R/renv/library/all.primer.tutorials-07f29d85/R-4.2/aarch64-apple-darwin20"
[2] "/Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/library"
```

Whenever you `library()` a package, R looks through, in order, the values returned by `.libPaths()`. **renv** sets things up so that the first item is its collection of packages. Note that these are placed in a weird location, including a temp directory named something like `all.primer.tutorials-07f29d85`. I am not sure how **renv** decided to create a new one of these. It does not delete the old ones. In the case of major breakdowns, deleting these directories by hand can fix things.


## Comments on .Rbuildignore

For some reason, it is impossible to include comments in the `.Rbuildignore`, at least by using the "#" symbol. I think the key issue is that using `*` (or maybe a parenthesis) in a line which begins with `#` causes trouble. Regardless, here are some thoughts on the version we currently use in **all.primer.tutorials**.

We would like to ensure that all the junk files which end up in the `tutorials/*` directories are not included in the build. Such files are often large. They also run the risk of messing things up in that they might cause certain tests to pass for us but which would fail for anyone who downloads from Github. (The `.gitignore` file does a reasonable job of ensuring that such files do not end up on Github.)

The key line is:

````
tutorials/[^/]*/(?!(data|images|.*Rmd))
````

This excludes everything in any subdirectory of the tutorials directory except an `images` directory (a file named images would also be included) or a `data` directory or a file suffixed with `.Rmd`.
